{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1478bdb",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# **코드 설명**\n",
    "\n",
    "---\n",
    "\n",
    "- 파 일 명 : 심장 질환 예측 경진대회 <br>\n",
    "- 시작날짜 : 2021.11.25 <br>\n",
    "- 수정날짜 : 2021.11.25 <br>\n",
    "- 작 성 자 : 김혁진 <br>\n",
    "- 작성주제 : Dacon / 심장 질환 예측 <br>\n",
    "\n",
    "--- \n",
    "\n",
    "- **순서** <br>\n",
    "  0. 기본설정 <br>\n",
    "    0.0. Google Drive Mount <br>\n",
    "    0.1. GPU 사용 <br>\n",
    "    0.2. Import Modules <br>\n",
    "    0.3. Initial Values\n",
    "    0.4. Set Off the Warning <br>\n",
    "    0.5. User Defined Function <br>\n",
    "\n",
    "  1. Data Load <br>\n",
    "\n",
    "  2. EDA\n",
    "  \n",
    "---\n",
    "\n",
    "- **참조**\n",
    "\n",
    "  (1) 대회 홈페이지 : [Dacon](https://dacon.io/competitions/official/235848/overview/description) <br>\n",
    "  (2) 하이퍼 파리미터 설명 : [Naver Blog](https://blog.naver.com/wideeyed/221333529176) <br>\n",
    "  (3) Class문 설명 : [Github](https://zzsza.github.io/development/2020/07/05/python-class/) <br>\n",
    "  (4) GPU 설정 : [Medium](https://medium.com/@am.sharma/lgbm-on-colab-with-gpu-c1c09e83f2af) <br>\n",
    "  (5) RAM 모두사용으로 세션다운 : [Tistory](https://somjang.tistory.com/entry/Google-Colab-%EC%9E%90%EC%A3%BC%EB%81%8A%EA%B8%B0%EB%8A%94-%EB%9F%B0%ED%83%80%EC%9E%84-%EB%B0%A9%EC%A7%80%ED%95%98%EA%B8%B0)\n",
    "\n",
    "---\n",
    "\n",
    "- **고려사항** <br>\n",
    "  (1) AutoEncoder로 파생변수 생성해보기 <br>\n",
    "  (2) 하이퍼파라미터 탐색 : grid-search, bayesian-optimization, [optuna](https://dacon.io/competitions/official/235713/codeshare/2704?page=1&dtype=recent)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8eb9c1",
   "metadata": {},
   "source": [
    "># **0. 기본설정**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c2214",
   "metadata": {},
   "source": [
    "## 0.1. Markdown : Tabular Left Align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624da7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f52129",
   "metadata": {},
   "source": [
    "## 0.2. Jupyter Notebook Style : Theme, Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # theme 설치\n",
    "# !pip install jupyterthemes\n",
    "\n",
    "# # jupyter notebook 최신버전\n",
    "# !pip install --upgrade notebook\n",
    "\n",
    "# # jupyter notebook 최신버전\n",
    "# !pip install --upgrade jupyterthemes\n",
    "\n",
    "# 0.2.1. 테마바꾸기(customizing)\n",
    "# !jt -t onedork -fs 115 -nfs 125 -tfs 115 -dfs 115 -ofs 115 -cursc r -cellw 80% -lineh 115 -altmd  -kl -T -N\n",
    "\n",
    "# 0.2.2. 쥬피터 노트북 화면 넓게 사용\n",
    "# 출처: https://taehooh.tistory.com/entry/Jupyter-Notebook-주피터노트북-화면-넓게-쓰는방법\n",
    "from IPython.core.display import display, HTML \n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cecfd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0.2.1 Google Drive Mount\n",
    "# # (Google Drive 사용 시 설정)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount = True) # 새로운 창에서 key 를 받아서 입력해야합니다. \n",
    "\n",
    "# # 0.2.1. 메모리 에러\n",
    "# https://growingsaja.tistory.com/477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 0.2.2. GPU 사용 (6분)\n",
    "# !git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "# !mkdir build\n",
    "# %cd /content/LightGBM\n",
    "# !cmake -DUSE_GPU=1 #avoid ..\n",
    "# !make -j$(nproc)\n",
    "# !sudo apt-get -y install python-pip\n",
    "# !sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
    "# %cd /content/LightGBM/python-package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e8d53",
   "metadata": {},
   "source": [
    "### Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall pandas -y\n",
    "# !pip uninstall numpy  -y\n",
    "# !pip uninstall lightgbm -y\n",
    "\n",
    "# !pip install pandas==1.1.0\n",
    "# !pip install numpy==1.21.2\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install lightgbm --install-option=--gpu\n",
    "\n",
    "# !pip install pandasql\n",
    "# !pip install seaborn\n",
    "# !pip install plotnine\n",
    "# !pip install pandasql\n",
    "\n",
    "# lightgbm 에러떴는데, 콘다에서 실행하면 해결됨\n",
    "# conda install -c conda-forge lightgbm \n",
    "\n",
    "# bayesian optimization 설치\n",
    "# !pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a02e69",
   "metadata": {},
   "source": [
    "## 0.3. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c6ebbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jupyter notebook 전용\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# basic modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "# value_counts() 범용적인 버전\n",
    "from collections import Counter as cnt\n",
    "\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7, 8.27)})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [11.7, 8.27] # [15, 10] # [11.7,8.27] - A4 size\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "# sqldf\n",
    "from pandasql import sqldf\n",
    "sql = lambda q: sqldf(q, globals())\n",
    "\n",
    "\n",
    "# modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# import lightgbm\n",
    "# !pip install lightgbm --install-option=--gpu --install-option=\"--opencl-include-dir=/usr/local/cuda/include/\" --install-option=\"--opencl-library=/usr/local/cuda/lib64/libOpenCL.so\"\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# color when print\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099b6ae",
   "metadata": {},
   "source": [
    "## 0.4. Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.4.1. Data Path\n",
    "# jupyter.notebook : 'os.getcwd() + '/DAT/블랙 프라이데이 판매 예측/''\n",
    "# google.colab     : '/content/drive/MyDrive/Python/4. 블랙프라이데이 판매예측/DAT/'\n",
    "DATA_PATH = os.getcwd() + '/DAT/2. 심장 질환 예측 경진대회 (Dacon)/'\n",
    "OUT_PATH  = os.getcwd() + '/OUT/2. 심장 질환 예측 경진대회 (Dacon)/'\n",
    "\n",
    "# 0.4.2. set seed\n",
    "SEED = 777\n",
    "\n",
    "# 0.4.3. lightgbm parameter\n",
    "# 처음 5회 랜덤 값으로 score 계산 후 45회 최적화\n",
    "INIT_POINTS = 15\n",
    "N_ITER = 15\n",
    "N_CV = 4\n",
    "EARLY_STOPPING_ROUNDS = 30\n",
    "\n",
    "# 0.4.4. scaling\n",
    "SCALE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd87120",
   "metadata": {},
   "source": [
    "## 0.5. Set Off the Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a3fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb769937",
   "metadata": {},
   "source": [
    "## 0.6. User Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a3e77b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 0.6.1. Seed Fix\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def seed_everything(seed: int = 1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    # torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    # torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "seed_everything(SEED)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 0.6.2. View all columns\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def View(data):\n",
    "\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    \n",
    "    print(data)\n",
    "\n",
    "    pd.set_option('display.max_rows', 0)\n",
    "    pd.set_option('display.max_columns', 0)\n",
    "    pd.set_option('display.width', 0)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 0.6.3. minmax function\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def minmax(x):\n",
    "    return min(x),max(x)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 0.6.4. 컬럼dict에서 target 제거\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - dict : 기준 dict\n",
    "# - key  : 삭제할 key\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def rmkey(dict, key):\n",
    "    tmp = dict.copy()\n",
    "    del tmp[key]\n",
    "    return tmp\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 0.6.5. 각 컬럼의 missing 개수를 파악하는 함수\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data     : 기준 data\n",
    "# - col_type : {column명 : type}로 이루어진 dictionary\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def missing_column_check(data, col_type):\n",
    "    num_na = []\n",
    "    for col_nm in data.columns:\n",
    "        data[col_nm] = data[col_nm].astype(col_type[col_nm])\n",
    "\n",
    "    # str인 경우에는 blank(공백)도 있는지 확인\n",
    "    if col_type[col_nm]=='str':\n",
    "        num_na_x = data[col_nm].str.strip().isnull().sum() + sum(data[col_nm].str.strip()=='')\n",
    "    \n",
    "    # numeric인 경우에는 null의 개수만 확인\n",
    "    else:\n",
    "        num_na_x = data[col_nm].isnull().sum()\n",
    "    \n",
    "    num_na = num_na + [num_na_x]\n",
    "\n",
    "    return(num_na)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 0.6.6. 교호작용항 추가\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data     : 기준 data\n",
    "# - num_vari : 숫자형 변수 list\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def interaction_term(data,num_vari):\n",
    "\n",
    "    num_var = list(set(num_vari) - set(['id']))\n",
    "\n",
    "    for i in range(0,len(num_var)):\n",
    "        for j in range(i,len(num_var)):\n",
    "            data[f'{num_var[i]}*{num_var[j]}'] = data[f'{num_var[i]}']*data[f'{num_var[j]}']\n",
    "\n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 0.6.7. density plot : histogram + density plot\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data : 기준 data\n",
    "# - vars : hist + kde를 그릴 숫자형 변수\n",
    "# - hue  : group화 변수\n",
    "# - binwidth_adj_ratio : binwidth 조정 비율\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def density_plot(data, vars, \n",
    "                 binwidths = None, hue = None,\n",
    "                 binwidth_adj_ratio = None):\n",
    "\n",
    "    from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "    # 1) vars가 1개뿐일 때 에러발생\n",
    "    #    -> 1개     : type = str\n",
    "    #    -> 2개이상 : type = ndarray, ...\n",
    "    if type(vars)==str:\n",
    "        vars = [vars]\n",
    "    \n",
    "    # 2) plotting (nrow,ncol) 설정\n",
    "    nrow = math.ceil(len(vars)**(1/2))\n",
    "    ncol = nrow\n",
    "\n",
    "    # 3) binwidths가 없을 때, binwidth 설정\n",
    "    # 출처 : http://www.aistudy.co.kr/paper/pdf/histogram_jeon.pdf\n",
    "    if binwidths is None:\n",
    "        binwidths = []\n",
    "        for col in data[vars].columns:\n",
    "            n_bin = math.ceil(1 + 3.32*math.log10(len(data)))\n",
    "            binwidth = ( data[col].max() - train[col].min() ) / n_bin\n",
    "            binwidths.append(binwidth)\n",
    "            del binwidth\n",
    "    \n",
    "    # 4) 설정한 binwidth를 조정하는 비율\n",
    "    if binwidth_adj_ratio is not None:\n",
    "        binwidths = [binwidth * binwidth_adj_ratio for binwidth in binwidths]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    # 5) vars 별로 plot 생성\n",
    "    for iter,var in enumerate(vars):\n",
    "        \n",
    "        binwidth = binwidths[iter]\n",
    "        \n",
    "        # (1) histogram\n",
    "        ax1 = fig.add_subplot(nrow, ncol, iter+1)\n",
    "        g1 = sns.histplot(data = data, x = var, hue = hue,\n",
    "                          kde = True, stat = 'probability', \n",
    "                          color = 'lightskyblue',\n",
    "                          binwidth = binwidth, ax = ax1)\n",
    "        ax2 = ax1.twinx()\n",
    "        \n",
    "        # (2) density plot\n",
    "        g2 = sns.kdeplot(data = data, x = var, hue = hue,\n",
    "                         color = 'red', lw = 2, ax = ax2)\n",
    "        ax2.set_ylim(0, ax1.get_ylim()[1] / binwidth)                  # similir limits on the y-axis to align the plots\n",
    "        #ax2.yaxis.set_major_formatter(PercentFormatter(1 / binwidth))  # show axis such that 1/binwidth corresponds to 100%\n",
    "        ax2.grid(False)\n",
    "        \n",
    "        # (3) density plot y축 없애기\n",
    "        g2.set(yticklabels=[]) \n",
    "        g2.set(ylabel=None)\n",
    "        g2.tick_params(right=False)\n",
    "        \n",
    "        a,b = divmod(iter,ncol)\n",
    "        if b!=0:\n",
    "            g1.set(ylabel=None)\n",
    "        \n",
    "    # 안겹치도록 설정\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# example : density_plot(train, vars=num_vari)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f20af",
   "metadata": {},
   "source": [
    "### 0.7. 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f33a6",
   "metadata": {},
   "source": [
    "># **1. Data**\n",
    "\n",
    "## 1.1. 변수정보 (변수명 참조 : [Dacon](https://dacon.io/competitions/official/235848/data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28384f",
   "metadata": {},
   "source": [
    "|변수명 | 변수정보 | 기준 | 변수상세 |\n",
    "|:---:|:---|:---|:---|\n",
    "| id | 데이터 고유 id | | |\n",
    "| age | 나이 | | |\n",
    "| sex | 성별  | 여자 = 0, 남자 = 1 | | |\n",
    "| cp | 가슴 통증 종류 | 무증상 = 0, 일반적이지 않은 협심증 = 1, 협심증이 아닌 통증 = 2, 일반적인 협심증 = 3 | | |\n",
    "| trestbps | 휴식 중 혈압(mmHg) | | | resting blood pressure |\n",
    "| chol | 혈중 콜레스테롤(mg/dl) | | serum cholestoral |\n",
    "| fbs | 공복 중 혈당 | 120 mg/dl 이하일 시 = 0, 초과일 시 = 1 | | fasting blood sugar |\n",
    "| restecg | 휴식 중 심전도 결과 | 좌심실비대증이 의심되거나 확실한 경우 = 0, 정상 = 1, having ST-T wave abnormality = 2 | resting electrocardiographic |\n",
    "| thalach | 최대 심박수 | | maximum heart rate achieved |\n",
    "| exang | 활동으로 인한  협심증 여부 | 없음 = 0, 있음 = 1 | exercise induced angina |\n",
    "| oldpeak | 휴식 대비 운동으로 인한 ST 하강 | | ST depression induced by exercise relative to rest |\n",
    "| slope | 활동 ST 분절 피크의 기울기 | 하강 = 0, 평탄 = 1, 상승 = 2 | the slope of the peak exercise ST segment |\n",
    "| ca | 형광 투시로 확인된 주요 혈관 수 | 0~3 개, <strong style=\"color:red\">Null값은 4로 인코딩됨</strong> | number of major vessels colored by flouroscopy |\n",
    "| thal | 지중해빈혈 여부 | 정상 = 1, 고정 결함 = 2, 가역 결함 = 3, <strong style=\"color:red\">Null값은 0으로 인코딩됨</strong> | thalassemia |\n",
    "| target | 심장 질환 진단 여부 | 혈관 지름 축소 50% 미만 = 0, 혈관 지름 축소 50% 이상 = 1 | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda586bc",
   "metadata": {},
   "source": [
    "## 1.2. Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c0dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COL_TYPE = {\n",
    "    'id' : int,\n",
    "    'age' : int,\n",
    "    'sex' : str,        # 0,1\n",
    "    'cp' : str,         # 0,1,2,3\n",
    "    'trestbps' : int,\n",
    "    'chol' : int,\n",
    "    'fbs' : str,        # 0,1\n",
    "    'restecg' : str,    # 0,1,2\n",
    "    'thalach' : int,\n",
    "    'exang' : str,      # 0,1\n",
    "    'oldpeak' : float,\n",
    "    'slope' : str,      # 0,1,2\n",
    "    'ca' : str,         # 0~3이고, Null=4\n",
    "    'thal' : str,       # 1,2,3이고, Null=0\n",
    "    'target' : str,     # 0,1\n",
    "}\n",
    "\n",
    "# Train Data Load (550,068 rows, 12 columns)\n",
    "train = pd.read_csv(DATA_PATH + 'train.csv', dtype = COL_TYPE)\n",
    "test  = pd.read_csv(DATA_PATH + 'test.csv', dtype = COL_TYPE)\n",
    "sub   = pd.read_csv(DATA_PATH + 'sample_submission.csv', dtype = COL_TYPE)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4ebf0",
   "metadata": {},
   "source": [
    "## 1.3. Missing Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0e065",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('> # of train Missing :', missing_column_check(train, COL_TYPE)[0])\n",
    "print('> # of test  Missing :', missing_column_check(test , COL_TYPE)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d1761",
   "metadata": {},
   "source": [
    "># **2. EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21bfe7",
   "metadata": {},
   "source": [
    "## 2.1. Characteristic Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d449a",
   "metadata": {},
   "source": [
    "## 2.2. Numeric Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0b992",
   "metadata": {},
   "source": [
    "## 2.3. Numeric Variable * Characteristic Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440262e",
   "metadata": {},
   "source": [
    "># **3. Segment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcce6f1",
   "metadata": {},
   "source": [
    "### segment를 구분하여 따로 모델 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df41380",
   "metadata": {},
   "source": [
    "># **4. Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(_df):\n",
    "    \n",
    "    df = _df.copy()\n",
    "    \n",
    "    return df\n",
    "\n",
    "train2 = preprocessing(train.copy())\n",
    "test2  = preprocessing(test .copy())\n",
    "\n",
    "# col type에 추가\n",
    "for str_var in []:\n",
    "    COL_TYPE[str_var] = str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a9a97",
   "metadata": {},
   "source": [
    "#### 교호작용항"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERACTION = False\n",
    "\n",
    "if INTERACTION is True:\n",
    "    train3 = interaction_term(train2,num_vari)\n",
    "    test3  = interaction_term(test2 ,num_vari)\n",
    "\n",
    "    for int_var in train3.columns[[col.find('*')>0 for col in train3.columns]]:\n",
    "        COL_TYPE[int_var] = int\n",
    "else:\n",
    "    train3 = train2.copy()\n",
    "    test3  = test2 .copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033d512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (1) 원-핫 인코딩\n",
    "def onehot_encoding(data, col_types):\n",
    "\n",
    "    raw_data = data.copy()\n",
    "    \n",
    "    cols = list(set(data.columns) - set(['target']))\n",
    "\n",
    "    for col in cols:\n",
    "        if col_types[col]==str:\n",
    "\n",
    "            data = pd.concat([\n",
    "                data.drop([col],axis=1).reset_index(drop=True),\n",
    "                pd.get_dummies(data[col], prefix = col).reset_index(drop=True).apply(lambda x:x.astype(int))\n",
    "                ],\n",
    "                axis=1)\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "# (2) str들 모두 int/category로 바꾸기\n",
    "def str_convert(data, col_types, convert = [int,'category']):\n",
    "\n",
    "    cols = list(set(data.columns) - set(['target']))\n",
    "\n",
    "    for col in cols:\n",
    "        if col_types[col]==str:\n",
    "            data[col] = data[col].astype(convert)\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "\n",
    "ONEHOT = False\n",
    "\n",
    "if ONEHOT:\n",
    "    train4 = onehot_encoding(data = train3, col_types = rmkey(COL_TYPE,'target'))\n",
    "    test4  = onehot_encoding(data = test3 , col_types = rmkey(COL_TYPE,'target'))\n",
    "else:\n",
    "    train4 = str_convert(train3, col_types = rmkey(COL_TYPE,'target'), convert = 'category')\n",
    "    test4  = str_convert(test3 , col_types = rmkey(COL_TYPE,'target'), convert = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d665ad",
   "metadata": {},
   "source": [
    "### 다른 level 있는 변수들 search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7925bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_category(data,col_types, ret=['dict','list']):\n",
    "\n",
    "    cols = list(set(data.columns) - set(['target']))\n",
    "\n",
    "    if ret=='dict':\n",
    "        len_cate = {}\n",
    "    elif ret=='list':\n",
    "        len_cate = []\n",
    "    else:\n",
    "        raise('error ret')\n",
    "        \n",
    "    for col in cols:\n",
    "        if col_types[col]==str:\n",
    "            _len = len(data[col].value_counts().index)\n",
    "            \n",
    "            if ret=='dict':\n",
    "                len_cate[col] = _len\n",
    "            elif ret=='list':\n",
    "                len_cate.append(_len)\n",
    "            \n",
    "    return(len_cate)\n",
    "\n",
    "check_category(train4,COL_TYPE,ret='dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60882259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_category2(data1,data2,col_types):\n",
    "\n",
    "    cols = list(set(data1.columns) - set(['target']))\n",
    "    max_char_len = max([len(x) if col_types[x]==str else 0 for x in col_types.keys()])\n",
    "    \n",
    "    # 없음\n",
    "    for col in cols:\n",
    "        if col_types[col]==str:\n",
    "            data1_cate = data1[col].value_counts().index.values.sort_values()\n",
    "            data2_cate = data2[col].value_counts().index.values.sort_values()\n",
    "            n_blank = (max_char_len-len(col))\n",
    "            if len(data1_cate)==len(data2_cate):\n",
    "                same_index =  data1_cate != data2_cate\n",
    "                print(col, ' '*n_blank, ':', same_index.sum())\n",
    "            else:\n",
    "                print(col, ' '*n_blank, ': differ length')\n",
    "            \n",
    "print(color.BOLD + color.BLUE + '> 다른 카테고리의 개수' + color.END)\n",
    "check_category2(train4,test4,COL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca456318",
   "metadata": {},
   "source": [
    "># **5. Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8aaa3e",
   "metadata": {},
   "source": [
    "### LGBM setting with optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac126d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# bayesian optimization에 쓰일 hyper parameter들의 boundary\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (100, 800), \n",
    "    'min_data_in_leaf': (0, 150),\n",
    "    'bagging_fraction' : (0.3, 0.9),\n",
    "    'feature_fraction' : (0.3, 0.9),\n",
    "    'min_child_weight': (0.01, 1.),   \n",
    "    'reg_alpha': (0.01, 1.), \n",
    "    'reg_lambda': (0.01, 1),\n",
    "    'max_depth':(6, 23),\n",
    "}\n",
    "\n",
    "# bayesian optimazation을 통하여 hyper parameter를 선택한\n",
    "# lightgbm modelling\n",
    "def build_lgb(x, y, val_x, val_y,\n",
    "              init_points=INIT_POINTS, n_iter=N_ITER, cv=N_CV, \n",
    "              ret_param=True, verbose=-1, is_test=False, \n",
    "              SEED=SEED):\n",
    "    \n",
    "    # verbose : 2 항상 출력, verbose = 1 최댓값일 때 출력, verbose = 0 출력 안함\n",
    "    \n",
    "    # (1) 각 hyper parameter들의 lgb model의 f1 score를 return\n",
    "    def LGB_bayesian(\n",
    "        num_leaves, \n",
    "        bagging_fraction,\n",
    "        feature_fraction,\n",
    "        min_child_weight, \n",
    "        min_data_in_leaf,\n",
    "        max_depth,\n",
    "        reg_alpha,\n",
    "        reg_lambda,\n",
    "         ):\n",
    "        # LightGBM expects next three parameters need to be integer. \n",
    "        num_leaves = int(num_leaves)\n",
    "        min_data_in_leaf = int(min_data_in_leaf)\n",
    "        max_depth = int(max_depth)\n",
    "\n",
    "        assert type(num_leaves) == int\n",
    "        assert type(min_data_in_leaf) == int\n",
    "        assert type(max_depth) == int\n",
    "\n",
    "        params = {\n",
    "            'num_leaves': num_leaves, \n",
    "            'min_data_in_leaf': min_data_in_leaf,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'bagging_fraction' : bagging_fraction,\n",
    "            'feature_fraction' : feature_fraction,\n",
    "            'learning_rate' : 0.05,\n",
    "            'max_depth': max_depth,\n",
    "            'reg_alpha': reg_alpha,\n",
    "            'reg_lambda': reg_lambda,\n",
    "            'objective': 'binary',\n",
    "            'save_binary': True,\n",
    "            'seed': SEED,\n",
    "            'feature_fraction_seed': SEED,\n",
    "            'bagging_seed': SEED,\n",
    "            'drop_seed': SEED,\n",
    "            'data_random_seed': SEED,\n",
    "            'boosting': 'gbdt', \n",
    "            'verbose': -1,\n",
    "            'verbose_eval': -1,\n",
    "            'boost_from_average': True,\n",
    "            'metric':'auc',\n",
    "            'n_estimators': 1000,\n",
    "            'n_jobs': -1,\n",
    "        }    \n",
    "\n",
    "        ## set reg options\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(x, y, eval_set=(val_x, val_y), early_stopping_rounds=30, verbose=0)\n",
    "        pred = model.predict(val_x)\n",
    "        score = metrics.f1_score(val_y, pred)\n",
    "        return score\n",
    "    \n",
    "    # (2) Get hyper parameter by bayesian optimazation\n",
    "    optimizer = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=SEED, verbose=-1)\n",
    "    init_points = init_points\n",
    "    n_iter = n_iter\n",
    "\n",
    "    # initial point, n_iter에 대해서 maximize 하는 bayesian optimazation 실행\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iter)\n",
    "    \n",
    "    # (3) bayesian optimazation를 통해서 얻은 hyper parameter\n",
    "    param_lgb = {\n",
    "        'min_data_in_leaf': int(optimizer.max['params']['min_data_in_leaf']), \n",
    "        'num_leaves': int(optimizer.max['params']['num_leaves']), \n",
    "        'learning_rate': 0.05,\n",
    "        'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "        'bagging_fraction': optimizer.max['params']['bagging_fraction'], \n",
    "        'feature_fraction': optimizer.max['params']['feature_fraction'],\n",
    "        'reg_lambda': optimizer.max['params']['reg_lambda'],\n",
    "        'reg_alpha': optimizer.max['params']['reg_alpha'],\n",
    "        'max_depth': int(optimizer.max['params']['max_depth']), \n",
    "        'objective': 'binary',\n",
    "        'save_binary': True,\n",
    "        'seed': SEED,\n",
    "        'feature_fraction_seed': SEED,\n",
    "        'bagging_seed': SEED,\n",
    "        'drop_seed': SEED,\n",
    "        'data_random_seed': SEED,\n",
    "        'boosting': 'gbdt', \n",
    "        'verbose': -1,\n",
    "        'boost_from_average': True,\n",
    "        'metric': 'binary_logloss', #'auc',\n",
    "        'n_estimators': 1000,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "\n",
    "    # final parameter\n",
    "    params = param_lgb.copy()\n",
    "    \n",
    "    # final model\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(x, y, eval_set=(val_x, val_y), early_stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=-1,\n",
    "              callbacks = [lgb.early_stopping(10, verbose=0), lgb.log_evaluation(period=0)])\n",
    "    \n",
    "    if ret_param:\n",
    "        return model, params\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6b3e3",
   "metadata": {},
   "source": [
    "### segment and dataset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a523a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca==cp > exang > slope > sex\n",
    "# > ca는 2,3건수가 너무적음\n",
    "# > cp는 1,3건수가 너무 적음\n",
    "seg_var = ['is_ca','is_cp','sex','exang','slope2']\n",
    "\n",
    "# 각 세그별 최소 건수\n",
    "[train4[seg_var_x].value_counts().min() for seg_var_x in seg_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2de58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "train_df = train4.copy()\n",
    "test_df  = test4 .copy()\n",
    "\n",
    "for seg_var_x in seg_var:\n",
    "\n",
    "    train_df[seg_var_x+'_pred'] = np.nan\n",
    "    test_df [seg_var_x+'_pred'] = np.nan\n",
    "\n",
    "    train_df[seg_var_x+'_tr_idx'] = np.nan\n",
    "\n",
    "    # segment별로 modelling\n",
    "    for iter in range(0,len(train_df[seg_var_x].value_counts().index)):\n",
    "\n",
    "        # segment setting\n",
    "        seg_var_value = train_df[seg_var_x].value_counts().index[iter]\n",
    "\n",
    "        # data setting\n",
    "        if seg_var_x is not None:\n",
    "            tr_seg_df = train_df[train_df[seg_var_x] == seg_var_value]\n",
    "            te_seg_df = test_df [test_df [seg_var_x] == seg_var_value]\n",
    "\n",
    "            drop_var = ['id','target'] +\\\n",
    "            [seg_var_x           for seg_var_x in seg_var] +\\\n",
    "            [seg_var_x+'_pred'   for seg_var_x in seg_var] +\\\n",
    "            [seg_var_x+'_tr_idx' for seg_var_x in seg_var]\n",
    "            \n",
    "            X_train = tr_seg_df[list(set(tr_seg_df.columns)-set(drop_var))]\n",
    "            X_test  = te_seg_df[list(set(te_seg_df.columns)-set(drop_var))]\n",
    "\n",
    "            y_train = tr_seg_df['target'][tr_seg_df[seg_var_x] == seg_var_value].astype(int).values\n",
    "\n",
    "        else:\n",
    "            tr_seg_df = train_df\n",
    "            te_seg_df = test_df\n",
    "\n",
    "            X_train = tr_seg_df.drop(['id','target'],axis=1)\n",
    "            X_test  = te_seg_df.drop(['id'         ],axis=1)\n",
    "\n",
    "            y_train = tr_seg_df['target'].astype(int).values\n",
    "\n",
    "        # modelling\n",
    "        n_fold = 5\n",
    "        sf = StratifiedKFold(n_fold, shuffle=True, random_state=SEED)\n",
    "\n",
    "        y_tr = []\n",
    "        y_te = []\n",
    "\n",
    "        c = 1\n",
    "        for tr_idx, val_idx in sf.split(X_train, y_train):\n",
    "            print(len(tr_idx), len(val_idx))\n",
    "            print('#'*25, f'CV {c}')\n",
    "\n",
    "            model, _ = build_lgb(X_train.iloc[tr_idx ], y_train[tr_idx ], \n",
    "                                 X_train.iloc[val_idx], y_train[val_idx],\n",
    "                                 init_points=INIT_POINTS, n_iter=N_ITER, cv=N_CV, \n",
    "                                 ret_param=True, is_test=False, \n",
    "                                 SEED=SEED)\n",
    "\n",
    "            y_tr_0 = model.predict(X_train)\n",
    "            y_te_0 = model.predict(X_test)\n",
    "\n",
    "            y_tr.append(y_tr_0)\n",
    "            y_te.append(y_te_0)\n",
    "\n",
    "            c += 1\n",
    "\n",
    "        # seg별 tr/val index 넣기\n",
    "        train_df[seg_var_x+'_tr_idx'] = ['1' if idx in tr_idx else '0' for idx in train.index]\n",
    "\n",
    "        # seg별 predict값 넣기\n",
    "        train_df[seg_var_x+'_pred'][train_df[seg_var_x] == seg_var_value] = np.where(np.mean(y_tr, 0)>0.5, 1, 0)\n",
    "        test_df [seg_var_x+'_pred'][test_df [seg_var_x] == seg_var_value] = np.where(np.mean(y_te, 0)>0.5, 1, 0)\n",
    "        \n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = (end_time-start_time)/60\n",
    "f'{runtime:.2f} Mins'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f39ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg_var_x in seg_var:\n",
    "    \n",
    "    all_pred = train_df[f'{seg_var_x}_pred']\n",
    "    all_true = train_df.target.astype(int).values\n",
    "    all_f1   = metrics.f1_score(all_pred,all_true)\n",
    "    \n",
    "    tr_pred = train_df[f'{seg_var_x}_pred'][train_df[f'{seg_var_x}_tr_idx']=='1']\n",
    "    tr_true = train_df.target[train_df[f'{seg_var_x}_tr_idx']=='1'].astype(int).values\n",
    "    tr_f1   = metrics.f1_score(tr_pred,tr_true)\n",
    "    \n",
    "    va_pred = train_df[f'{seg_var_x}_pred'][train_df[f'{seg_var_x}_tr_idx']=='0']\n",
    "    va_true = train_df.target[train_df[f'{seg_var_x}_tr_idx']=='0'].astype(int).values\n",
    "    va_f1   = metrics.f1_score(va_pred,va_true)\n",
    "    \n",
    "    print('-'*50)\n",
    "    print(f'{seg_var_x} - all : {all_f1:.2f}, train : {tr_f1:.2f}, valid : {va_f1:.2f}')\n",
    "#     print(pd.crosstab(all_pred,all_true))\n",
    "#     print(pd.crosstab(tr_pred,tr_true))\n",
    "    print(pd.crosstab(va_pred,va_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1e05c",
   "metadata": {},
   "source": [
    "#### train_df 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv(OUT_PATH + 'train_df(2).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e3479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30599c7b",
   "metadata": {},
   "source": [
    "#### segment 별 f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7cc501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "max_char_len = max([len(seg_var_x) for seg_var_x in seg_var])\n",
    "\n",
    "\n",
    "for seg_var_x in seg_var:\n",
    "\n",
    "    var_pred = seg_var_x+'_pred'\n",
    "    tr_idx   = train_df[seg_var_x+'_tr_idx']=='1'\n",
    "    va_idx   = train_df[seg_var_x+'_tr_idx']=='0'\n",
    "    \n",
    "    tr_pred = np.array([1 if pred==1 else 0 for pred in train_df[var_pred][tr_idx]])\n",
    "    tr_true = train_df.target[tr_idx].astype(int).values\n",
    "\n",
    "    va_pred = np.array([1 if pred==1 else 0 for pred in train_df[var_pred][va_idx]])\n",
    "    va_true = train_df.target[va_idx].astype(int).values\n",
    "\n",
    "    tr_f1_score = metrics.f1_score(tr_pred,tr_true)\n",
    "    va_f1_score = metrics.f1_score(va_pred,va_true)\n",
    "    \n",
    "    n_blank = ' '*(max_char_len-len(seg_var_x))\n",
    "    \n",
    "    if seg_var_x==seg_var[0]: print(' '*max_char_len, '  train   valid')\n",
    "    print(f'{seg_var_x} {n_blank} {tr_f1_score: .3f}  {va_f1_score: .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ef8f4",
   "metadata": {},
   "source": [
    "#### 각 seg별 f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_list = []\n",
    "for seg_var_x in seg_var:\n",
    "    f1_score = metrics.f1_score(train_df[[seg_var_x + '_pred']],train_df.target.astype(int).values)\n",
    "    n_blank = ' '*(max_char_len-len(seg_var_x))\n",
    "    \n",
    "    if seg_var_x==seg_var[0]: print(' '*max_char_len, 'f1_score')\n",
    "    print(f'{seg_var_x} {n_blank} {f1_score: .3f}')\n",
    "    \n",
    "    f1_score_list.append(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c446a4",
   "metadata": {},
   "source": [
    "#### 각 segment의 weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0215155",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [f/sum(f1_score_list) for f in f1_score_list]\n",
    "for seg,w in zip(seg_var,weight):\n",
    "    n_blank = ' '*(max_char_len-len(seg))\n",
    "    if seg==seg_var[0]: print(' '*max_char_len, '  weight')\n",
    "    print(f'{seg} {n_blank} {w*100: .1f}%')\n",
    "\n",
    "print('\\n',weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9417b",
   "metadata": {},
   "source": [
    "#### weighted predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) mixed\n",
    "tr_pred_mix = train_df[[seg + '_pred' for seg in seg_var]].sum(axis=1)/len(seg_var)\n",
    "tr_f1_score_mix = metrics.f1_score(np.where(tr_pred_mix>0.5,1,0), train_df.target.astype(int).values)\n",
    "\n",
    "# (2) weighted mixed\n",
    "tr_pred_weighted_mix = np.array([train_df[seg+'_pred'].astype(int).values*weight[iter] \n",
    "                                 for iter,seg in enumerate(['is_ca', 'is_cp', 'sex', 'exang', 'slope2'])]).sum(axis=0)\n",
    "tr_f1_score_weighted_mix = metrics.f1_score(np.where(tr_pred_weighted_mix>0.5,1,0), train_df.target.astype(int).values)\n",
    "\n",
    "print(f'f1_score of          mixed segment predicted value:{tr_f1_score_mix         : .3f}')\n",
    "print(f'f1_score of weighted mixed segment predicted value:{tr_f1_score_weighted_mix: .3f}')\n",
    "\n",
    "# a=pd.DataFrame({'x' : np.where(tr_pred_mix>0.5,1,0),\n",
    "#                 'y' : np.where(tr_pred_weighted_mix>0.5,1,0)})\n",
    "# pd.crosstab(a.x,a.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tr_pred_mix'] = np.where(train_df[[seg + '_pred' for seg in seg_var]].sum(axis=1)/len(seg_var)>0.5, '1', '0')\n",
    "test_df ['te_pred_mix'] = np.where(test_df [[seg + '_pred' for seg in seg_var]].sum(axis=1)/len(seg_var)>0.5, '1', '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba92e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['te_pred_mix'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429901fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target'] = ['1' if pred=='1' else '0' for pred in test_df['te_pred_mix']]\n",
    "sub.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ba370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 점수 : 0.92537\n",
    "sub.to_csv(OUT_PATH + 'sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
